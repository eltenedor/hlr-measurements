Sender: LSF System <lsfadmin@hpa0691>
Subject: Job 234261: <INTEL_256_2048_64_new> in cluster <lichtenberg> Exited

Job <INTEL_256_2048_64_new> was submitted from host <hla0003> by user <gu08vomo> in cluster <lichtenberg>.
Job was executed on host(s) <16*hpa0691>, in queue <short>, as user <gu08vomo> in cluster <lichtenberg>.
                            <16*hpa0577>
                            <16*hpa0580>
                            <16*hpa0651>
                            <16*hpa0607>
                            <16*hpa0661>
                            <16*hpa0582>
                            <16*hpa0611>
                            <16*hpa0590>
                            <16*hpa0545>
                            <16*hpa0547>
                            <16*hpa0639>
                            <16*hpa0552>
                            <16*hpa0629>
                            <16*hpa0699>
                            <16*hpa0648>
</home/gu08vomo> was used as the home directory.
</home/gu08vomo/performance/256_2048_op3_speedup_hp_new> was used as the working directory.
Started at Sun Nov  9 23:37:12 2014
Results reported at Sun Nov  9 23:37:42 2014

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#! /bin/sh

#BSUB -J INTEL_256_2048_64_new
#BSUB -u fabian.gabel@stud.tu-darmstadt.de
##BSUB -B -N

#BSUB -e /home/gu08vomo/output/INTEL/256_2048_op3_speedup_hp_new/256_2048_64.err.%J
#BSUB -o /home/gu08vomo/output/INTEL/256_2048_op3_speedup_hp_new/256_2048_64.out.%J

#BSUB -n 256
#BSUB -W 0:10
#BSUB -x

#BSUB -a openmpi

module load openmpi/intel/1.8.2
export PETSC_DIR=/home/gu08vomo/soft/petsc/3.5.2/build/arch-openmpi-opt-intel-hlr
export MYWORKDIR=/work/scratch/gu08vomo/performance/256_2048_op3_speedup_hp_new/256_2048_64/
export PETSC_OPS="-pressure_sub_pc_type icc -momentum_ksp_converged_reason -pressure_ksp_converged_reason"

echo "PETSC_DIR="$PETSC_DIR
echo "MYWORKDIR="$MYWORKDIR

cd ${MYWORKDIR}
pwd
#get newest caffa
cd caffa.MTM
#git checkout -- .
#git pull origin master
cd caffa3d.MB
# prepare binary
cp ../../param3d.inc .
make opt-intel-analytical EXPATH=${MYWORKDIR}
#run the case
cd ${MYWORKDIR}
cp ../control.cin .
mpirun -report-bindings -map-by ppr:4:node -map-by ppr:2:socket -n 64 ./caffa3d.MB.lnx ${PETSC_OPS} -log_summary /home/gu08vomo/output/INTEL/256_2048_op3_speedup_hp_new/256_2048_64.log.${LSB_JOBID}

------------------------------------------------------------

Exited with exit code 24.

Resource usage summary:

    CPU time :               50.29 sec.
    Max Memory :             3 MB
    Average Memory :         3.00 MB
    Total Requested Memory : -
    Delta Memory :           -
    (Delta: the difference between total requested memory and actual max usage.)
    Max Swap :               160 MB

    Max Processes :          10
    Max Threads :            10

The output (if any) follows:

PETSC_DIR=/home/gu08vomo/soft/petsc/3.5.2/build/arch-openmpi-opt-intel-hlr
MYWORKDIR=/work/scratch/gu08vomo/performance/256_2048_op3_speedup_hp_new/256_2048_64/
/work/scratch/gu08vomo/performance/256_2048_op3_speedup_hp_new/256_2048_64
mpif90 caffa3d.MB.f -fpp -O3 -DUSE_INTEL_COMPILER -traceback -mcmodel=medium -DUSE_ANALYTICAL -I/home/gu08vomo/soft/petsc/3.5.2/build/arch-openmpi-opt-intel-hlr/include -IsolverSIP -I. -o /work/scratch/gu08vomo/performance/256_2048_op3_speedup_hp_new/256_2048_64/caffa3d.MB.lnx -Wl,-rpath=/home/gu08vomo/soft/petsc/3.5.2/build/arch-openmpi-opt-intel-hlr/lib -L/home/gu08vomo/soft/petsc/3.5.2/build/arch-openmpi-opt-intel-hlr/lib -lpetsc -DUSE_INTERPOLATION
  ENTER PROBLEM NAME (SIX CHARACTERS):  
 ****************************************************
 NAME OF PROBLEM SOLVED control
 
 ****************************************************
-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code.. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 3 in communicator MPI_COMM_WORLD 
with errorcode 59.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpirun detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[60459,1],21]
  Exit code:    24
--------------------------------------------------------------------------


PS:

Read file </home/gu08vomo/output/INTEL/256_2048_op3_speedup_hp_new/256_2048_64.err.234261> for stderr output of this job.

